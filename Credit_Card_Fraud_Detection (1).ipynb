{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Overview\n",
        "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
        "\n",
        "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
        "\n",
        "Inspiration Identify fraudulent credit card transactions.\n",
        "\n",
        "Given the class imbalance ratio, we are measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification."
      ],
      "metadata": {
        "id": "3O55UdeD3TA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "YGq93hcD3YVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data can be downloaded from https://www.kaggle.com/mlg-ulb/creditcardfraud/downloads/creditcardfraud.zip/3\n",
        "df= pd.read_csv('/content/Credit card fraud detection.zip')"
      ],
      "metadata": {
        "id": "f-H34KEl3Uki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "J-DX0Ei73kYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Null Value Check\n",
        "df.isnull().values.any()"
      ],
      "metadata": {
        "id": "Q3CPryEt3n39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Class Balance Check\n",
        "print('Fraud Percentage: {}'.format(round((df['Class'].value_counts()[1]/len(df))*100,2)))\n",
        "print('Non Fraud Percentage: {}'.format(round((df['Class'].value_counts()[0]/len(df))*100,2)))"
      ],
      "metadata": {
        "id": "wWkig4UN3n7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count= df['Class'].value_counts()\n",
        "count.plot(kind='bar')\n",
        "plt.xticks(range(2),['Non Fraud','Fraud'])\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CHaeC0s23n9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How imbalanced is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!"
      ],
      "metadata": {
        "id": "r34Fd54L3wxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#\n",
        "#Distributions:\n",
        "\n",
        "By seeing the distributions we can have an idea how skewed are these features, we can also see further distributions of the other features."
      ],
      "metadata": {
        "id": "y7d9O3KV32Nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax= plt.subplots(2,1, figsize=(20,10))\n",
        "\n",
        "amount= df['Amount'].values\n",
        "time= df['Time'].values\n",
        "\n",
        "sns.distplot(amount,ax=ax[0], color='r')\n",
        "sns.distplot(time,ax=ax[1],color='b')"
      ],
      "metadata": {
        "id": "AQZONR7Q3n_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By distribution we can see transaction amounts are very small, where as time is distributed."
      ],
      "metadata": {
        "id": "D6Lq0dvW4Ahn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scaling\n",
        "As data is given after PCA to hide original data so scalling was done on the variables except time and amount which we will scale"
      ],
      "metadata": {
        "id": "5mo06iVx4C7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler # it is prone to outliers\n",
        "ss1= RobustScaler()\n",
        "df['Amount']= ss1.fit_transform(df['Amount'].values.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "5PaQbfFW3oEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss2= RobustScaler()\n",
        "df['Time']= ss2.fit_transform(df['Time'].values.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "VCs6iXXr3oI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Y5HgZNz44QRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Splitting the Data (Original DataFrame)\n",
        "Before proceeding with the Random UnderSampling technique we have to separate the orginal dataframe. Why? for testing purposes, remember although we are splitting the data when implementing Random UnderSampling or OverSampling techniques, we want to test our models on the original testing set not on the testing set created by either of these techniques. The main goal is to fit the model either with the dataframes that were undersample and oversample (in order for our models to detect the patterns), and test it on the original testing set."
      ],
      "metadata": {
        "id": "5IM9MsYs4WBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xorg=df.drop('Class',axis=1)\n",
        "yorg= df.loc[:,'Class']"
      ],
      "metadata": {
        "id": "bxGdK38Y4QUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xorgtrain,xorgtest,yorgtrain,yorgtest= train_test_split(xorg,yorg,test_size=0.2,random_state=9)"
      ],
      "metadata": {
        "id": "g4XKhaUX4QXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(xorgtrain.shape,xorgtest.shape,yorgtrain.shape,yorgtest.shape)"
      ],
      "metadata": {
        "id": "oMZoZpSp4QZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Sampling\n",
        "we will implement \"Random Under Sampling\" which basically consists of removing data in order to have a more balanced dataset and thus avoiding our models to overfitting.\n",
        "\n",
        "Note: The main issue with \"Random Under-Sampling\" is that we run the risk that our classification models will not perform as accurate as we would like to since there is a great deal of information loss (bringing 492 non-fraud transaction from 284,315 non-fraud transaction)"
      ],
      "metadata": {
        "id": "zfEvlUkI4hdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using imblearn library\n",
        "# from imblearn.under_sampling import NearMiss\n",
        "\n",
        "# nm=NearMiss(random_state=9)\n",
        "# xr,yr= nm.fit_sample(xorg,yorg)\n",
        "\n",
        "# from collections import Counter\n",
        "# print('Original Count: {}'.format(Counter(yorg)))\n",
        "# print('Sampled Count: {}'.format(Counter(yr)))\n",
        "\n",
        "# # Now we have equal fraud and non fraud data.\n",
        "\n",
        "# new_df= pd.concat([pd.DataFrame(xr,columns=xorg.columns),pd.DataFrame(yr)],axis=1)\n",
        "\n",
        "# new_df= new_df.rename({0:'Class'},axis=1)\n",
        "\n",
        "# new_df.head()"
      ],
      "metadata": {
        "id": "TuY_qIWh4QcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using shuffling and selecting first 492 non fraud"
      ],
      "metadata": {
        "id": "XDnpC22o4sdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n",
        "\n",
        "# Lets shuffle the data before creating the subsamples\n",
        "\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# amount of fraud classes 492 rows.\n",
        "fraud_df = df.loc[df['Class'] == 1]\n",
        "non_fraud_df = df.loc[df['Class'] == 0][:492] #Taking top 492 row for 0\n",
        "\n",
        "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
        "\n",
        "# Shuffle dataframe rows\n",
        "new_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
        "\n",
        "new_df.head()"
      ],
      "metadata": {
        "id": "I1qDQvn_4QeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Oversampling using RandomOverSampler"
      ],
      "metadata": {
        "id": "p8SJ4e804y88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from imblearn.over_sampling import RandomOverSampler\n",
        "# nm= RandomOverSampler(ratio=1,random_state=42)\n",
        "# xr,yr= nm.fit_sample(xorg,yorg)\n",
        "\n",
        "# from collections import Counter\n",
        "# print('Original Count: {}'.format(Counter(yorg)))\n",
        "# print('Sampled Count: {}'.format(Counter(yr)))\n",
        "\n",
        "# # Now we have equal fraud and non fraud data.\n",
        "\n",
        "# new_df= pd.concat([pd.DataFrame(xr,columns=xorg.columns),pd.DataFrame(yr)],axis=1)\n",
        "\n",
        "# new_df= new_df.rename({0:'Class'},axis=1)\n",
        "\n",
        "# new_df.head()"
      ],
      "metadata": {
        "id": "Xm1rOASG41m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.shape"
      ],
      "metadata": {
        "id": "xN_etfhz462_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "vVIiAZFS49eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Correlation Matrices\n",
        "Correlation matrices are the essence of understanding our data. We want to know if there are features that influence heavily in whether a specific transaction is a fraud. However, it is important that we use the correct dataframe (subsample) in order for us to see which features have a high positive or negative correlation with regards to fraud transactions."
      ],
      "metadata": {
        "id": "bkpKUX8R5BNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for Original Data frame\n",
        "plt.figure(figsize=(20,20))\n",
        "\n",
        "sns.heatmap(df.corr(),annot=True,cmap='coolwarm_r')"
      ],
      "metadata": {
        "id": "PThcgRzU5AW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For new sampled df\n",
        "#for Original Data frame\n",
        "plt.figure(figsize=(20,20))\n",
        "\n",
        "sns.heatmap(new_df.corr(),annot=True,cmap='coolwarm_r')"
      ],
      "metadata": {
        "id": "TfqWzwrT5Gon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative Correlations: V17,V16, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.\n",
        "Positive Correlations: V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction.\n",
        "BoxPlots: We will use boxplots to have a better understanding of the distribution of these features in fradulent and non fradulent transactions."
      ],
      "metadata": {
        "id": "aI4X34fE5Kds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Negative Correlation"
      ],
      "metadata": {
        "id": "-Ky_yQw45R0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, axes = plt.subplots(ncols=5, figsize=(20,4))\n",
        "\n",
        "# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\n",
        "sns.boxplot(x=\"Class\", y=\"V17\", data=new_df, ax=axes[0])\n",
        "axes[0].set_title('V17 vs Class Negative Correlation')\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V16\", data=new_df, ax=axes[1])\n",
        "axes[1].set_title('V16 vs Class Negative Correlation')\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V14\", data=new_df, ax=axes[2])\n",
        "axes[2].set_title('V14 vs Class Negative Correlation')\n",
        "\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V12\", data=new_df, ax=axes[3])\n",
        "axes[3].set_title('V12 vs Class Negative Correlation')\n",
        "\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=axes[4])\n",
        "axes[4].set_title('V10 vs Class Negative Correlation')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uPFEZwTC5VA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Positive Correlation"
      ],
      "metadata": {
        "id": "YEeTACPk5Yef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
        "\n",
        "# Postive Correlations with our Class (The higher our feature value the more likely it will be a fraud transaction)\n",
        "sns.boxplot(x=\"Class\", y=\"V2\", data=new_df, ax=axes[0])\n",
        "axes[0].set_title('V2 vs Class Positive Correlation')\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V4\", data=new_df, ax=axes[1])\n",
        "axes[1].set_title('V4 vs Class Positive Correlation')\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V11\", data=new_df, ax=axes[2])\n",
        "axes[2].set_title('V11 vs Class Positive Correlation')\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V19\", data=new_df, ax=axes[3])\n",
        "axes[3].set_title('V2 vs Class Positive Correlation')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4JS3BvLd5dRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neg= ['V17','V16','V14','V12','V10']\n",
        "\n",
        "f, axes = plt.subplots(ncols=len(neg), figsize=(20,4))\n",
        "for i,j in enumerate(neg):\n",
        "# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\n",
        "    sns.boxplot(x=\"Class\", y=j, data=new_df, ax=axes[i])\n",
        "    axes[i].set_title(j + ' vs Class Negative Correlation')"
      ],
      "metadata": {
        "id": "OFrs9D135mag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos= ['V2','V4','V11','V19']\n",
        "\n",
        "f, axes = plt.subplots(ncols=len(pos), figsize=(20,4))\n",
        "for i,j in enumerate(pos):\n",
        "# Postive Correlations with our Class (The higher our feature value the more likely it will be a fraud transaction)\n",
        "    sns.boxplot(x=\"Class\", y=j, data=new_df, ax=axes[i])\n",
        "    axes[i].set_title(j+'vs Class Positive Correlation')"
      ],
      "metadata": {
        "id": "rdG_kjQ559G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boxplots are a standardized way of displaying the distribution of data based on a five number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”).\n",
        "\n",
        "median (Q2/50th Percentile): the middle value of the dataset.\n",
        "\n",
        "first quartile (Q1/25th Percentile): the middle number between the smallest number (not the “minimum”) and the median of the dataset.\n",
        "\n",
        "third quartile (Q3/75th Percentile): the middle value between the median and the highest value (not the “maximum”) of the dataset.\n",
        "\n",
        "interquartile range (IQR): 25th to the 75th percentile.\n",
        "\n",
        "whiskers (shown in blue)\n",
        "\n",
        "outliers (shown as green circles)\n",
        "\n",
        "“maximum”: Q3 + 1.5*IQR\n",
        "\n",
        "“minimum”: Q1 -1.5*IQR\n",
        "\n",
        "'outliers= 3* IQR or more than that"
      ],
      "metadata": {
        "id": "spe4MksE6FsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Anomly Detection\n",
        "visualize Distributions: We first start by visualizing the distribution of the feature we are going to use to eliminate some of the outliers. V14 is the only feature that has a Gaussian distribution compared to features V12 and V10.\n",
        "Determining the threshold: After we decide which number we will use to multiply with the iqr (the lower more outliers removed), we will proceed in determining the upper and lower thresholds by substrating q25 - threshold (lower extreme threshold) and adding q75 + threshold (upper extreme threshold).\n",
        "Conditional Dropping: Lastly, we create a conditional dropping stating that if the \"threshold\" is exceeded in both extremes, the instances will be removed.\n",
        "Boxplot Representation: Visualize through the boxplot that the number of \"extreme outliers\" have been reduced to a considerable amount."
      ],
      "metadata": {
        "id": "DHWf59eb6H2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n",
        "\n",
        "v14_fraud_dist = new_df['V14'].loc[new_df['Class'] == 1].values\n",
        "sns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color='#FB8861')\n",
        "ax1.set_title('V14 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
        "\n",
        "v12_fraud_dist = new_df['V12'].loc[new_df['Class'] == 1].values\n",
        "sns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')\n",
        "ax2.set_title('V12 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
        "\n",
        "\n",
        "v10_fraud_dist = new_df['V10'].loc[new_df['Class'] == 1].values\n",
        "sns.distplot(v10_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')\n",
        "ax3.set_title('V10 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-JbSQFZR6Kr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Outliers Treatment\n",
        "as we say in box plots all the variables with coorelation have outliers so now we will treat them in with iqr , lb and ub."
      ],
      "metadata": {
        "id": "r5YNt4LW6OsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2= new_df # I am creating the copy of new_df to preserve the original data\n",
        "treat= ['V14','V12','V10']\n",
        "for j in treat:\n",
        "    q25,q75= new_df[j].quantile(q=0.25),new_df[j].quantile(q=0.75)\n",
        "    iqr= q75-q25\n",
        "    cut_off= iqr*1.5\n",
        "    lb,ub= q25-cut_off,q75+cut_off\n",
        "    outliers= [x for x in new_df[j] if x<=lb or x>=ub]\n",
        "    print(j,'Q25: {} , Q75: {}, IQR: {}, Cutoff: {}, LB: {}, UB: {},'.format(q25,q75,iqr,cut_off,lb,ub))\n",
        "    print(len(outliers), outliers)\n",
        "    df2= df2.drop(df2[(df2['V14'] > ub) | (df2['V14']< lb)].index, axis=0)\n",
        "    print(df2.shape)\n",
        "    print('----' * 44)\n",
        "\n",
        "\n",
        "# Tried imputing outliers.\n",
        "#     lb,ub= new_df[j].quantile(q=0.05),new_df[j].quantile(q=0.95)\n",
        "#     outliers= [x for x in new_df[j] if x>ub or x<lb]\n",
        "#     print(len(outliers),outliers)\n",
        "#     func= (lambda x: x if x>=ub or x<=lb else new_df[j].mean())\n",
        "#     df2[j]= df2[j].apply(func)"
      ],
      "metadata": {
        "id": "9jWkrDzl6SrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.shape"
      ],
      "metadata": {
        "id": "haihuuul6VFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter(df2['Class'])"
      ],
      "metadata": {
        "id": "AWjjyFCu6XFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, axes = plt.subplots(ncols=len(treat), figsize=(20,4))\n",
        "for i,j in enumerate(treat):\n",
        "# Postive Correlations with our Class (The higher our feature value the more likely it will be a fraud transaction)\n",
        "    sns.boxplot(x=\"Class\", y=j, data=df2, ax=axes[i])\n",
        "    axes[i].set_title(j)"
      ],
      "metadata": {
        "id": "broH93uI6Zl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "x=df2.drop('Class',axis=1).values\n",
        "y= df2.loc[:,'Class'].values\n",
        "\n",
        "# SPlitting the test and train after removing outliers\n",
        "\n",
        "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "0UoNs-KQ6iHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fitting the models and calculating test and training score"
      ],
      "metadata": {
        "id": "EKU7VwUd6mSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier= {\n",
        "    'Logistic Regression':LogisticRegression(),\n",
        "    'KNN':KNeighborsClassifier(),\n",
        "    'SVC':SVC(),\n",
        "    'DecisionTree':DecisionTreeClassifier()\n",
        "}\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "RzGrQ0JO6rff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key,values in classifier.items():\n",
        "    values.fit(xtrain,ytrain)\n",
        "    training_score= cross_val_score(values,xtrain,ytrain,cv=5)\n",
        "    print('Training accuracy score of {} is {}'.format(key,round(training_score.mean()*100,2)))\n",
        "    train_pred = cross_val_predict(values, xtrain, ytrain, cv=5)\n",
        "    print('Roc_Auc training score for {} is {}: '.format(key, round(roc_auc_score(ytrain,train_pred)*100,2)))\n",
        "    test_score= cross_val_score(values,xtest,ytest,cv=5)\n",
        "    print('Test accuracy score of {} is {}'.format(key,round(test_score.mean()*100,2)))\n",
        "    test_pred = cross_val_predict(values, xtest, ytest, cv=5)\n",
        "    print('Roc_Auc test score for {} is {}: '.format(key, round(roc_auc_score(ytest,test_pred)*100,2)))\n",
        "    print('---'*30)"
      ],
      "metadata": {
        "id": "ugAqzG5B6uHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculating for original data"
      ],
      "metadata": {
        "id": "pMluB6Dj6xxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key,values in classifier.items():\n",
        "    values.fit(xtrain,ytrain)\n",
        "    test_score= cross_val_score(values,xorgtest,yorgtest,cv=5)\n",
        "    print('Test accuracy score of {} is {}'.format(key,round(test_score.mean()*100,2)))\n",
        "    test_pred = cross_val_predict(values, xorgtest,yorgtest, cv=5)\n",
        "    print('Roc_Auc test score for {} is {}: '.format(key, round(roc_auc_score(yorgtest,test_pred)*100,2)))\n",
        "    print('---'*30)"
      ],
      "metadata": {
        "id": "hBwRa7ls61Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyper Parameter Tuning using GridSearchCv"
      ],
      "metadata": {
        "id": "4hsx3paD67iP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GridSearchCV to find the best parameters.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# Logistic Regression\n",
        "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
        "knears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
        "svc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
        "tree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)),\n",
        "              \"min_samples_leaf\": list(range(5,7,1))}\n",
        "\n",
        "\n",
        "classifier= {\n",
        "    'Logistic Regression':LogisticRegression(),\n",
        "    'KNN':KNeighborsClassifier(),\n",
        "    'SVC':SVC(),\n",
        "    'DecisionTree':DecisionTreeClassifier()\n",
        "}\n",
        "\n",
        "def grid_search(classifier,Param):\n",
        "    grid_log_reg = GridSearchCV(classifier,param_grid=Param)\n",
        "    grid_log_reg.fit(xtrain, ytrain)\n",
        "    best_param = grid_log_reg.best_estimator_\n",
        "    print('{} algorithm best parameter are : {}'.format(classifier.__class__.__name__,best_param))\n",
        "\n",
        "\n",
        "\n",
        "grid_search(LogisticRegression(),log_reg_params)"
      ],
      "metadata": {
        "id": "tyqJnTfU6_9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_reg = LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
        "                             intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
        "                             multi_class='ovr', n_jobs=None, penalty='l1',\n",
        "                             random_state=None, solver='liblinear', tol=0.0001,\n",
        "                             verbose=0, warm_start=False)\n",
        "log_reg.fit(xtrain,ytrain)"
      ],
      "metadata": {
        "id": "0qfajRrL_zeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_reg_pred = cross_val_predict(log_reg, xtest, ytest, cv=5,method=\"decision_function\")"
      ],
      "metadata": {
        "id": "gT2anpj1Arqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "fpr,tpr,threshold= roc_curve(ytest,log_reg_pred)"
      ],
      "metadata": {
        "id": "6l-NHrzVAvGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "plt.plot([0,1],[0,1],'r--')\n",
        "plt.plot(fpr,tpr,'g')\n",
        "plt.title('Auc Score is :'+str(auc(fpr,tpr)))"
      ],
      "metadata": {
        "id": "HjHKJ6aZAyBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dWJIauR6BE7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deeper look into the logistic regression classifier."
      ],
      "metadata": {
        "id": "daQSe4Sq_FlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "True Positives: Correctly Classified Fraud Transactions\n",
        "False Positives: Incorrectly Classified Fraud Transactions\n",
        "Negative: Correctly Classified Non-Fraud Transactions\n",
        "False Negative: Incorrectly Classified Non-Fraud Transactions\n",
        "Precision (1-Specificty): True Positives/(True Positives + False Positives) i.e ratio of correct predicted positive to the total predicted positive.\n",
        "Recall or Sensitivity: True Positives/(True Positives + False Negatives) i.e what percentage of fraud is correctly identified\n",
        "F1 Score = 2*(Recall * Precision) / (Recall + Precision) is the weighted average of Precision and Recall.\n",
        "Accuracy = TP+TN/TP+FP+FN+TN\n",
        "= TP/TP+FN\n",
        "Specificity= TN/TN+FP i.e what percent is of non fraud is correctly identified\n",
        "Precision as the name says, says how precise (how sure) is our model in detecting fraud transactions while recall is the amount of fraud cases our model is able to detect.\n",
        "\n",
        "Precision/Recall Tradeoff: The more precise (selective) our model is, the less cases it will detect. Example: Assuming that our model has a precision of 95%, Let's say there are only 5 fraud cases in which the model is 95% precise or more that these are fraud cases. Then let's say there are 5 more cases that our model considers 90% to be a fraud case, if we lower the precision there are more cases that our model will be able to detect."
      ],
      "metadata": {
        "id": "Pb_SwSff_JyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
        "\n",
        "# Define and configure the Logistic Regression model\n",
        "log_reg = LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
        "                             intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
        "                             multi_class='ovr', n_jobs=None, penalty='l1',\n",
        "                             random_state=None, solver='liblinear', tol=0.0001,\n",
        "                             verbose=0, warm_start=False)\n",
        "\n",
        "# Fit the model with the training data\n",
        "log_reg.fit(xtrain, ytrain)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = log_reg.predict(xtest)\n",
        "\n",
        "# Calculate and print evaluation metrics\n",
        "print('Recall Score: {:.2f}'.format(recall_score(ytest, y_pred)))\n",
        "print('Precision Score: {:.2f}'.format(precision_score(ytest, y_pred)))\n",
        "print('F1 Score: {:.2f}'.format(f1_score(ytest, y_pred)))\n",
        "print('Accuracy Score: {:.2f}'.format(accuracy_score(ytest, y_pred)))\n"
      ],
      "metadata": {
        "id": "Iom7Lg8kBHNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing on orginal dataset"
      ],
      "metadata": {
        "id": "_Ij3Slr-BPJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
        "y_pred = log_reg.predict(xorgtest)\n",
        "\n",
        "print('Recall Score: {:.2f}'.format(recall_score(yorgtest, y_pred)))\n",
        "print('Precision Score: {:.2f}'.format(precision_score(yorgtest, y_pred)))\n",
        "print('F1 Score: {:.2f}'.format(f1_score(yorgtest, y_pred)))\n",
        "print('Accuracy Score: {:.2f}'.format(accuracy_score(yorgtest, y_pred)))"
      ],
      "metadata": {
        "id": "3AAbNDhWBS87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "so while predicting on original test values it has an unaccepted accuracy which shows our model is over fitted. This over fitting occured because we did the sampling before cross validating.\n",
        "\n",
        "So we will do undersampling during cross validation, which will be correct value for down sampling"
      ],
      "metadata": {
        "id": "mvQIvpf7BWo7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Overfitting during Cross Validation:¶\n",
        "In our undersample analysis I want to show you a common mistake I made that I want to share with all of you. It is simple, if you want to undersample or oversample your data you should not do it before cross validating. Why because you will be directly influencing the validation set before implementing cross-validation causing a \"data leakage\" problem.you will see amazing precision and recall scores but in reality our data is overfitting!\n",
        "\n",
        "Below I am doing undersampling during cross validation but still accuracy wont be good, as in undersampling we loose the information."
      ],
      "metadata": {
        "id": "iUkef-MDBb7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#if we get the minority class (\"Fraud) in our case, and create the synthetic points before cross validating we have a certain influence on the \"validation set\" of the cross validation process. Remember how cross validation works, let's assume we are splitting the data into 5 batches, 4/5 of the dataset will be the training set while 1/5 will be the validation set. The test set should not be touched! For that reason, we have to do the creation of synthetic datapoints \"during\" cross-validation and not before, just like below:"
      ],
      "metadata": {
        "id": "IGXwMOdUBhZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
        "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, roc_auc_score\n",
        "\n",
        "# Assuming df is your DataFrame containing the data\n",
        "undersample_X = df.drop('Class', axis=1)\n",
        "undersample_y = df['Class']\n",
        "\n",
        "sss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
        "\n",
        "for train_index, test_index in sss.split(undersample_X, undersample_y):\n",
        "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
        "    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n",
        "    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n",
        "\n",
        "undersample_Xtrain = undersample_Xtrain.values\n",
        "undersample_Xtest = undersample_Xtest.values\n",
        "undersample_ytrain = undersample_ytrain.values\n",
        "undersample_ytest = undersample_ytest.values\n",
        "\n",
        "undersample_accuracy = []\n",
        "undersample_precision = []\n",
        "undersample_recall = []\n",
        "undersample_f1 = []\n",
        "undersample_auc = []\n",
        "\n",
        "# Implementing NearMiss Technique\n",
        "# Distribution of NearMiss (Just to see how it distributes the labels we won't use these variables)\n",
        "X_nearmiss, y_nearmiss = NearMiss().fit_resample(undersample_X.values, undersample_y.values)\n",
        "print('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n",
        "\n",
        "# Define parameter grid for Logistic Regression\n",
        "log_reg_params = {\n",
        "    \"penalty\": ['l1', 'l2'],\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "    'solver': ['liblinear', 'saga'],  # Only solvers that support 'l1' penalty\n",
        "    'multi_class': ['ovr', 'auto']  # Valid values for multi_class\n",
        "}\n",
        "\n",
        "rand_log_reg1 = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n",
        "\n",
        "# Cross Validating the right way\n",
        "for train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n",
        "    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), rand_log_reg1)\n",
        "    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n",
        "    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n",
        "\n",
        "    undersample_accuracy.append(undersample_pipeline.score(undersample_Xtrain[test], undersample_ytrain[test]))\n",
        "    undersample_precision.append(precision_score(undersample_ytrain[test], undersample_prediction))\n",
        "    undersample_recall.append(recall_score(undersample_ytrain[test], undersample_prediction))\n",
        "    undersample_f1.append(f1_score(undersample_ytrain[test], undersample_prediction))\n",
        "    undersample_auc.append(roc_auc_score(undersample_ytrain[test], undersample_prediction))\n",
        "\n",
        "print('--' * 45)\n",
        "print(\"accuracy: {}\".format(np.mean(undersample_accuracy)))\n",
        "print(\"precision: {}\".format(np.mean(undersample_precision)))\n",
        "print(\"recall: {}\".format(np.mean(undersample_recall)))\n",
        "print(\"f1: {}\".format(np.mean(undersample_f1)))\n",
        "print(\"AUC: {}\".format(np.mean(undersample_auc)))\n",
        "print('--' * 45)\n"
      ],
      "metadata": {
        "id": "_ZG55xCzB-Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "labels = ['No Fraud', 'Fraud']\n",
        "nm_prediction = rand_log_reg1.best_estimator_.predict(undersample_Xtest)\n",
        "print(classification_report(undersample_ytest, nm_prediction, target_names=labels))"
      ],
      "metadata": {
        "id": "xZT0wzhfCImq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best= rand_log_reg1.best_estimator_\n",
        "y_score = best.decision_function(xorgtest)\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "average_precision = average_precision_score(yorgtest, y_score)\n",
        "\n",
        "print('Average precision-recall score: {0:0.2f}'.format(\n",
        "      average_precision))\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precision, recall, threshold = precision_recall_curve(yorgtest, y_score)\n",
        "\n",
        "plt.step(recall, precision, color='r', alpha=0.2,\n",
        "         where='post')\n",
        "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
        "                 color='#F59B00')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('Under Sampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n",
        "          average_precision), fontsize=16)"
      ],
      "metadata": {
        "id": "v55I01WgCMLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now we will increase our data points so that we dont losse any informations\n",
        "\n",
        "SMOTE stands for Synthetic Minority Over-sampling Technique. Unlike Random UnderSampling, SMOTE creates new synthetic points in order to have an equal balance of the classes. This is another alternative for solving the \"class imbalance problems\".\n",
        "\n",
        "Understanding SMOTE:\n",
        "\n",
        "Solving the Class Imbalance: SMOTE creates synthetic points from the minority class in order to reach an equal balance between the minority and majority class.\n",
        "Location of the synthetic points: SMOTE picks the distance between the closest neighbors of the minority class, in between these distances it creates synthetic points.\n",
        "Final Effect: More information is retained since we didn't have to delete any rows unlike in random undersampling.\n",
        "Accuracy || Time Tradeoff: Although it is likely that SMOTE will be more accurate than random under-sampling, it will take more time to train since no rows are eliminated as previously stated.\n",
        "SMOTE will be done \"during\" cross validation and not \"prior\" to the cross validation process. Synthetic data are created only for the training set without affecting the validation set."
      ],
      "metadata": {
        "id": "Gocg3c6RCRE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
        "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "print('Length of X (train): {} | Length of y (train): {}'.format(len(xorgtrain), len(yorgtrain)))\n",
        "print('Length of X (test): {} | Length of y (test): {}'.format(len(xorgtest), len(yorgtest)))\n",
        "\n",
        "# List to append the score and then find the average\n",
        "accuracy_lst = []\n",
        "precision_lst = []\n",
        "recall_lst = []\n",
        "f1_lst = []\n",
        "auc_lst = []\n",
        "\n",
        "# Classifier with optimal parameters\n",
        "# log_reg_sm = LogisticRegression()\n",
        "rand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n",
        "\n",
        "sss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
        "# Implementing SMOTE Technique\n",
        "# Cross Validating the right way\n",
        "# Parameters\n",
        "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
        "for train, test in sss.split(xorgtrain, yorgtrain):\n",
        "    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg)\n",
        "    # SMOTE happens during Cross Validation not before..\n",
        "    model = pipeline.fit(xorgtrain.iloc[train], yorgtrain.iloc[train])\n",
        "    best_est = rand_log_reg.best_estimator_\n",
        "    prediction = best_est.predict(xorgtrain.iloc[test])\n",
        "\n",
        "    accuracy_lst.append(pipeline.score(xorgtrain.iloc[test], yorgtrain.iloc[test]))\n",
        "    precision_lst.append(precision_score(yorgtrain.iloc[test], prediction))\n",
        "    recall_lst.append(recall_score(yorgtrain.iloc[test], prediction))\n",
        "    f1_lst.append(f1_score(yorgtrain.iloc[test], prediction))\n",
        "    auc_lst.append(roc_auc_score(yorgtrain.iloc[test], prediction))\n",
        "\n",
        "print('---' * 45)\n",
        "print('')\n",
        "print(\"accuracy: {}\".format(np.mean(accuracy_lst)))\n",
        "print(\"precision: {}\".format(np.mean(precision_lst)))\n",
        "print(\"recall: {}\".format(np.mean(recall_lst)))\n",
        "print(\"f1: {}\".format(np.mean(f1_lst)))\n",
        "print('---' * 45)"
      ],
      "metadata": {
        "id": "E3uENzVfCUIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "labels = ['No Fraud', 'Fraud']\n",
        "smote_prediction = best_est.predict(xorgtest)\n",
        "print(classification_report(yorgtest, smote_prediction, target_names=labels))"
      ],
      "metadata": {
        "id": "T9b6-mQGYbCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_score = best_est.decision_function(xorgtest)\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "average_precision = average_precision_score(yorgtest, y_score)\n",
        "\n",
        "print('Average precision-recall score: {0:0.2f}'.format(\n",
        "      average_precision))\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precision, recall, threshold = precision_recall_curve(yorgtest, y_score)\n",
        "\n",
        "plt.step(recall, precision, color='r', alpha=0.2,\n",
        "         where='post')\n",
        "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
        "                 color='#F59B00')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n",
        "          average_precision), fontsize=16)"
      ],
      "metadata": {
        "id": "TSFIqOfsYet2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Neural network testing for Under and Over Sample data."
      ],
      "metadata": {
        "id": "m7UM4JSUYkCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am making simple two hidden layer network and will use it"
      ],
      "metadata": {
        "id": "vqQHEpovYniD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "xRCPf5cSYo9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier= Sequential()\n",
        "classifier.add(Dense(15, activation='relu',kernel_initializer='uniform',input_shape=(30,)))\n",
        "classifier.add(Dense(15, activation='relu',kernel_initializer='uniform'))\n",
        "classifier.add(Dense(1, activation='sigmoid', kernel_initializer='uniform' ))"
      ],
      "metadata": {
        "id": "yGvTvhRPYtox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.summary()"
      ],
      "metadata": {
        "id": "JULlfwv6YxYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.compile(optimizer='adam',loss= ['binary_crossentropy'],metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "j9yGtsUQY_Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import NearMiss\n",
        "\n",
        "# Assuming df is your DataFrame containing the data\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Applying NearMiss technique to undersample the data\n",
        "X_nearmiss, y_nearmiss = NearMiss().fit_resample(X, y)\n",
        "\n",
        "# Assuming classifier is a neural network model (e.g., from Keras)\n",
        "classifier.fit(X_nearmiss, y_nearmiss, batch_size=10, epochs=100)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Define a simple neural network model\n",
        "classifier = Sequential()\n",
        "classifier.add(Dense(units=16, activation='relu', input_dim=X_nearmiss.shape[1]))\n",
        "classifier.add(Dense(units=8, activation='relu'))\n",
        "classifier.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model with the undersampled data\n",
        "classifier.fit(X_nearmiss, y_nearmiss, batch_size=10, epochs=100)\n"
      ],
      "metadata": {
        "id": "ix0MzyTVZhL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "undersample_pred_prob = classifier.predict(xorgtest, batch_size=200, verbose=0)"
      ],
      "metadata": {
        "id": "lvUUaPS6Z5ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Predict on the test set and convert probabilities to class labels\n",
        "undersample_pred = (classifier.predict(xorgtest, batch_size=200, verbose=0) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculate and print the confusion matrix\n",
        "conf_matrix = confusion_matrix(yorgtest, undersample_pred)\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "JrvJ4tozEHHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "undersample_pred = (classifier.predict(xorgtest, batch_size=200, verbose=0) > 0.5).astype(\"int32\")\n",
        "conf_matrix = confusion_matrix(yorgtest, undersample_pred)\n"
      ],
      "metadata": {
        "id": "-UsyH8Itaoco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(yorgtest, undersample_pred, target_names=labels))"
      ],
      "metadata": {
        "id": "tnwwZWoLa9Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, threshold = precision_recall_curve(yorgtest, undersample_pred)\n",
        "\n",
        "plt.step(recall, precision, color='r', alpha=0.2,\n",
        "         where='post')\n",
        "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
        "                 color='#F59B00')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n",
        "          average_precision_score(yorgtest, undersample_pred)), fontsize=16)"
      ],
      "metadata": {
        "id": "dNKQ_UAWbBWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
        "Xsm_train, ysm_train = sm.fit_resample(X, y)\n",
        "\n",
        "\n",
        "# SMOTE Technique (OverSampling) After splitting and Cross Validating\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
        "Xsm_train, ysm_train = sm.fit_resample(X, y)\n",
        "\n",
        "# Assuming classifier is a neural network model (e.g., from Keras)\n",
        "classifier.fit(Xsm_train, ysm_train, batch_size=200, epochs=100)"
      ],
      "metadata": {
        "id": "py1lIKORbVCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oversample_pred = (classifier.predict(xorgtest, batch_size=200, verbose=0) > 0.5).astype(\"int32\")\n",
        "conf_matrix = confusion_matrix(yorgtest, oversample_pred)\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "id": "xLOuV_c-eP4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, threshold = precision_recall_curve(yorgtest, oversample_pred)\n",
        "\n",
        "plt.step(recall, precision, color='r', alpha=0.2,\n",
        "         where='post')\n",
        "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
        "                 color='#F59B00')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n",
        "          average_precision_score(yorgtest, oversample_pred)), fontsize=16)"
      ],
      "metadata": {
        "id": "dw5iqyMgeWMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(yorgtest, oversample_pred, target_names=labels))"
      ],
      "metadata": {
        "id": "ojhjTGQ2eZyX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}